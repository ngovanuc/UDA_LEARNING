{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chapter 03: Writing Web Crawler\n",
    "Date: 18/04/2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thu thập thông tin toàn bộ trang web dùng đệ quy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "\n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen(\"http://en.wikipedia.org{}\".format(articleUrl))\n",
    "    bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "    return bs.find('div', {'id': 'bodyContent'}).find_all(\n",
    "        'a',\n",
    "        href=re.compile('^(/wiki/)((?!:).*$)')\n",
    "        )\n",
    "\n",
    "links = getLinks('/wiki/Kevin_Bacon')\n",
    "while len(links) > 0:\n",
    "    newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "    print(newArticle)\n",
    "    links = getLinks(newArticle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***note***\n",
    "việc thu thập tất cả các đường link trong một hệ thống web, và tiếp tục truy\n",
    "vết các đường link con từ những đường link đã lấy được trước đó\n",
    "\n",
    "=> điều này tạo thành một vòng lặp đệ quy\n",
    "một hệ thống web mỗi trang có 10 link và có độ sâu là 5 thì đường đương với\n",
    "5^10 cho tất cả các đường link được tìm thấy. tuy nhiên hiếm thấy trang web nào có thể đạt tới 100.000 đường link.\n",
    "\n",
    "khi quét một hệ thống web dùng đệ quy, chắc chắn có thể bị lặp lại vô hạn do bị trùng đường link được tìm thấy. điều này có thể giải quyết bằng cách lưu lại và các đường link mới được sưu tầm phải được kiểm tra xem đã tồn tại trước đó hay chưa!\n",
    "\n",
    "python giới hạn với vòng lặp đệ quy là 1.000 vòng. trừ khi chúng ta phải thiết lập một bộ đếm đệ quy khác cho nó.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  một ví dụ đơn giản về chương trình đệ quy cho việc quét toàn bộ web\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "pages = set()\n",
    "\n",
    "def getLinks(pageUrl):\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    for link in bs.find_add('a', href=re.compile('^(/wiki)')):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages:\n",
    "                # we have encountered a new page\n",
    "                newPage = link.attrs['href']\n",
    "                print(newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)\n",
    "\n",
    "getLinks('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thu thập dữ liệu trên toàn bộ trang web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "pages = set()\n",
    "\n",
    "def getLinks(pageUrl):\n",
    "    global pages\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    try:\n",
    "        print(bs.h1.get_text())\n",
    "        print(bs.find(id='mw-content-text').find_all('p')[0])\n",
    "        print(bs.find(id='ca-edit').find('span')\n",
    "              .find('a').attrs['href'])\n",
    "    except AttributeError:\n",
    "        print('This page is missing something! Contunuing...')\n",
    "    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages:\n",
    "                newPage = link.attrs['href']\n",
    "                print('_'*20)\n",
    "                print(newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)\n",
    "\n",
    "getLinks('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***chuyển hướng***\n",
    "\n",
    "- chuyển hướng máy chủ: URL được thay đổi trước khi tải\n",
    "\n",
    "- chuyển hướng máy khách: nhìn thấy thông báo \"bạn sẽ được chuyển hướng sau xxx giây\". thư viện url của python 3.x sẽ tự động giải quyết, nếu sử dụng thư viện request, cần phải đặt cờ cho nó: allow_redirects=True.\n",
    "\n",
    "ví dụ: r=requests.get('http://github.com',allow_redirects=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***khi cố gắng thu thập tất cả các liên kết ngoài, hãy tự hỏi***\n",
    "\n",
    "- cố gắng thu thập dữ liệu gì?\n",
    "=> phương hướng giải quyết là thử nghiệm chạy trên một vài trang web chỉ định trước\n",
    "\n",
    "- trình thu thập của tôi có khám phá các trang web mới mà tôi không biết?\n",
    "\n",
    "- liên kết đến một trang web mới hay là đi sâu vào trang web hiện tại?\n",
    "\n",
    "- điều kiện để không truy cập vào các trang web không mong muốn cụ thể?\n",
    "\n",
    "- có quan tâm đến ngôn ngữ của nội dung hay không?\n",
    "\n",
    "- làm thế nào để bảo vệ mình trước hành động pháp lí nếu trình thu thập thông tin web của tôi thu hút sự chú ý của quản trị viên trên một trang web mà nó chạy qua?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JOS UC\\AppData\\Local\\Temp\\ipykernel_23556\\3373549060.py:2: DeprecationWarning: Seeding based on hashing is deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version. The only \n",
      "supported seed types are: None, int, float, str, bytes, and bytearray.\n",
      "  random.seed(datetime.datetime.now())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random external link is: https://www.linkedin.com/company/oreilly-media\n",
      "HTTP Error 999: Request denied\n"
     ]
    }
   ],
   "source": [
    "pages = set()\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "# truy xuất danh sách tất cả các liên kết nội bộ được tìm thấy trên một trang\n",
    "def getInternalLinks(bs, includeUrl):\n",
    "    includeUrl = '{}://{}'.format(urlparse(includeUrl).scheme,\n",
    "                                  urlparse(includeUrl).netloc)\n",
    "    internalLinks = []\n",
    "    # tìm tất cả links bắt đầu bằng '/'\n",
    "    for link in bs.find_all('a',\n",
    "                            href=re.compile('^(/|.*'+includeUrl+')')):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in internalLinks:\n",
    "                if (link.attrs['href'].startswith('/')):\n",
    "                    internalLinks.append(includeUrl+link.attrs['href'])\n",
    "                else:\n",
    "                    internalLinks.append(link.attrs['href'])\n",
    "    return internalLinks\n",
    "\n",
    "\n",
    "# truy xuất danh sách tất cả các liên kết bên ngoài được tìm thấy trên một trang\n",
    "def getExternalLinks(bs, excludeUrl):\n",
    "    externalLinks = []\n",
    "    # tìm tất cả các link bắt đầu với 'http' mà không chứa URL hiện tại\n",
    "    for link in bs.find_all('a',\n",
    "                            href=re.compile('^(http|www)((?!'+excludeUrl+').)*$')):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in externalLinks:\n",
    "                externalLinks.append(link.attrs['href'])\n",
    "    return externalLinks\n",
    "\n",
    "\n",
    "def getRandomExternalLink(startingPage):\n",
    "    # trong trường hợp lỗi 403: Forbiden. hãy sử dụng người dùng giả mạo    \n",
    "    html = urlopen(startingPage)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    externalLinks = getExternalLinks(bs, \n",
    "                                     urlparse(startingPage).netloc)\n",
    "    if len(externalLinks) == 0:\n",
    "        print(\"No external links, looking around the site for one!\")\n",
    "        domain = '{}://{}'.format(urlparse(startingPage).scheme,\n",
    "                                  urlparse(startingPage).netloc)\n",
    "        internalLinks = getInternalLinks(bs, domain)\n",
    "        return getRandomExternalLink(internalLinks[random.randint(0, len(internalLinks)-1)])\n",
    "    else:\n",
    "        return externalLinks[random.randint(0, len(externalLinks)-1)]\n",
    "    \n",
    "\n",
    "def followExternalonly(startingSite):\n",
    "    externalLink = getRandomExternalLink(startingSite)\n",
    "    print(\"Random external link is: {}\".format(externalLink))\n",
    "    try:\n",
    "        followExternalonly(externalLink)\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "followExternalonly('http://oreilly.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***HTTPError: HTTP Error 999: Request denied***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.oreilly.com\n",
      "https://www.oreilly.com/member/login/\n",
      "https://www.oreilly.com/online-learning/try-now.html\n",
      "https://www.oreilly.com/online-learning/teams.html\n",
      "https://www.oreilly.com/online-learning/government.html\n",
      "https://www.oreilly.com/online-learning/academic.html\n",
      "https://www.oreilly.com/online-learning/individuals.html\n",
      "https://www.oreilly.com/online-learning/features.html\n",
      "https://www.oreilly.com/online-learning/courses.html\n",
      "https://www.oreilly.com/online-learning/feature-certification.html\n",
      "https://www.oreilly.com/online-learning/intro-interactive-learning.html\n",
      "https://www.oreilly.com/online-learning/live-events.html\n",
      "https://www.oreilly.com/online-learning/feature-answers.html\n",
      "https://www.oreilly.com/online-learning/insights-dashboard.html\n",
      "https://www.oreilly.com/radar/\n",
      "https://www.oreilly.com/content-marketing-solutions.html\n",
      "https://learning.oreilly.com/start-trial/\n",
      "https://www.oreilly.com/about/oreilly-approach-to-generative-ai.html\n",
      "https://www.oreilly.com/online-learning/generative-ai.html\n",
      "https://www.oreilly.com/online-learning/testimonials.html\n",
      "https://www.oreilly.com/diversity/scholarship-program.html\n",
      "https://www.oreilly.com/online-learning/oreilly-awards-winners-2023.html\n",
      "https://learning.oreilly.com/search/?query=author%3A%22Arianne%20Dee%22&extended_publisher_data=true&highlight=true&include_assessments=false&include_case_studies=true&include_courses=true&include_playlists=true&include_collections=true&include_notebooks=true&include_sandboxes=true&include_scenarios=true&is_academic_institution_account=false&source=suggestion&sort=date_added&facet_json=true&json_facets=true&page=0&include_facets=false\n",
      "https://learning.oreilly.com/search/?query=author%3A%22Bruno%20Gon%C3%A7alves%22&extended_publisher_data=true&highlight=true&include_assessments=false&include_case_studies=true&include_courses=true&include_playlists=true&include_collections=true&include_notebooks=true&include_sandboxes=true&include_scenarios=true&is_academic_institution_account=false&source=user&sort=date_added&facet_json=true&json_facets=true&page=0&include_facets=false\n",
      "https://learning.oreilly.com/search/?query=author%3A%22Kelsey%20Hightower%22&extended_publisher_data=true&highlight=true&include_assessments=false&include_case_studies=true&include_courses=true&include_playlists=true&include_collections=true&include_notebooks=true&include_sandboxes=true&include_scenarios=true&is_academic_institution_account=false&source=user&sort=date_added&facet_json=true&json_facets=true&page=0&include_facets=false\n",
      "https://learning.oreilly.com/search/?query=author%3A%22Sari%20Greene%22&extended_publisher_data=true&highlight=true&include_assessments=false&include_case_studies=true&include_courses=true&include_playlists=true&include_collections=true&include_notebooks=true&include_sandboxes=true&include_scenarios=true&is_academic_institution_account=false&source=user&sort=date_added&facet_json=true&json_facets=true&page=0&include_facets=false\n",
      "https://learning.oreilly.com/search/?query=author%3A%22Neal%20Ford%22&extended_publisher_data=true&highlight=true&include_assessments=false&include_case_studies=true&include_courses=true&include_playlists=true&include_collections=true&include_notebooks=true&include_sandboxes=true&include_scenarios=true&is_academic_institution_account=false&source=user&sort=date_added&facet_json=true&json_facets=true&page=0&include_facets=false\n",
      "https://learning.oreilly.com/search/?query=author%3A%22Ken%20Kousen%22&extended_publisher_data=true&highlight=true&include_assessments=false&include_case_studies=true&include_courses=true&include_playlists=true&include_collections=true&include_notebooks=true&include_sandboxes=true&include_scenarios=true&is_academic_institution_account=false&source=user&sort=date_added&facet_json=true&json_facets=true&page=0&include_facets=false\n",
      "https://www.oreilly.com/online-learning/live-online-sessions.html\n",
      "https://www.oreilly.com/online-learning/enterprise.html\n",
      "https://www.linkedin.com/company/oreilly-media\n",
      "https://www.youtube.com/user/OreillyMedia\n",
      "https://www.oreilly.com/anz/\n",
      "https://oreilly.hk/\n",
      "https://oreillylearning.in/\n",
      "https://oreilly.id/\n",
      "https://www.oreilly.co.jp/index.shtml\n",
      "https://itunes.apple.com/us/app/safari-to-go/id881697395\n",
      "https://play.google.com/store/apps/details?id=com.safariflow.queue\n",
      "https://channelstore.roku.com/details/c8a2d0096693eb9455f6ac165003ee06/oreilly\n",
      "https://www.amazon.com/OReilly-Media-Inc/dp/B087YYHL5C/ref=sr_1_2?dchild=1&keywords=oreilly&qid=1604964116&s=mobile-apps&sr=1-2\n",
      "https://www.oreilly.com/privacy.html?donotsell=show\n",
      "https://www.oreilly.com/about/history.html\n",
      "https://www.oreilly.com/tim/\n",
      "https://www.oreilly.com/pub/pr/3452\n",
      "https://www.oreilly.com/pub/pr/3451\n",
      "https://www.oreilly.com/pub/pr/3444\n",
      "https://www.oreilly.com/press/\n",
      "https://www.oreilly.com/about/editorial_independence.html\n",
      "https://www.oreilly.com/about/resources.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001ECAF4AF9A0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\JOS UC\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 788, in _clean_thread_parent_frames\n",
      "    if phase != \"start\":\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "#  lấy tất cả danh sách của URL bên ngoài được tìm thấy trên một site\n",
    "allExtLinks = set()\n",
    "allIntLinks = set()\n",
    "\n",
    "def getAllExternalLinks(siteUrl):\n",
    "    html = urlopen(siteUrl)\n",
    "    domain = '{}://{}'.format(urlparse(siteUrl).scheme,\n",
    "                              urlparse(siteUrl).netloc)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    internalLinks = getInternalLinks(bs, domain)\n",
    "    externalLinks = getExternalLinks(bs, domain)\n",
    "\n",
    "    for link in externalLinks:\n",
    "        if link not in allExtLinks:\n",
    "            allExtLinks.add(link)\n",
    "            print(link)\n",
    "    for link in internalLinks:\n",
    "        if link not in allIntLinks:\n",
    "            getAllExternalLinks(link)\n",
    "\n",
    "\n",
    "allIntLinks.add('http://oreily.com')\n",
    "getAllExternalLinks('http://oreilly.com')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
