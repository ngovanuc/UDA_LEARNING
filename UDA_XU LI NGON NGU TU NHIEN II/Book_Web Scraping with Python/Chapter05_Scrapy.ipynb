{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chapter 05: Scrapy\n",
    "Date: 25/05/2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initializing a new spider\n",
    "\n",
    "To create a new spider in the current directory, run the following from the commandline:\n",
    "\n",
    "- scrapy startproject wikiSpider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## writing a simple scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To create a crawler, you will add a new file inside the spiders directory \n",
    "at wikiSpider/wikiSpider/spiders/article.py. In your newly created article.py \n",
    "file, write the following:'''\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ArticleSpider(scrapy.Spider):\n",
    "    name = 'article'\n",
    "\n",
    "    def start_request(self):\n",
    "        urls = [\n",
    "            # tại đây sẽ sử dụng wikipedia để ví dụ\n",
    "            'http://en.wikipedia.org/wiki/Python_''%28programming_language%29','https://en.wikipedia.org/wiki/Functional_programming','https://en.wikipedia.org/wiki/Monty_Python'\n",
    "        ]\n",
    "        return [scrapy.Request(url=url, callback=self.parse) for url in urls]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        url = response.url\n",
    "        title = response.css('h1::text').extract_first()\n",
    "        print(f'URL is: {url}')\n",
    "        print(f'Title is: {title}')\n",
    "\n",
    "# run this:\n",
    "# scrapy runspider article.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spidering with rules\n",
    "\n",
    "khung Scrapy không thể dễ dàng chạy từ bên trong sổ ghi chép Jupyter, khiến cho tiến trình tuyến tính của mã khó nắm bắt được"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "\n",
    "\n",
    "class ArticleSpider(CrawlSpider):\n",
    "    name = 'article'\n",
    "    allowed_domains = ['wikipedia.org']\n",
    "    start_urls = ['https://en.wikipedia.org/wiki/Benevolent_dictator_for_life']\n",
    "    rules = [Rule(LinkExtractor(allow=r'.*'), callback='parse_items', follow=True)]\n",
    "\n",
    "    def parse_items(self, response):\n",
    "        url = response.url\n",
    "        title = response.css('h1::text').extract_first()\n",
    "        text = response.xpath('//div[@id=\"mw-content-text\"]//text()').extract_first()\n",
    "        lastUpdated = lastUpdated.replace(\n",
    "            'This page was last edited on ', ''\n",
    "        )\n",
    "        print('URL is: {}'.format(url))\n",
    "        print('title is: {} '.format(title))\n",
    "        print('text is: {}'.format(text))\n",
    "        print('Last updated: {}'.format(lastUpdated))\n",
    "\n",
    "'''\n",
    "một Rule có thể được truyền vào 6 tham số:\n",
    "    - link_extractor: đối số bắt buộc duy nhất đối tượng LinkExtractor\n",
    "    - callback: hàm nên được sử dụng để phân tích nội dung trên page\n",
    "    - cb_kwargs: một từ điển các đối số được truyền đến hàm callback với {arg_name1: arg_value1, arg_name2: arg_value2}\n",
    "    và nó có thể hữu ích để sử dụng lại các chức năng phân tích cú pháp giống nhau cho các tác vụ hơi khác nhau\n",
    "    - follow: cho biết liệu có muốn đưa liên kết được tìm thấy tại trang đó vào trình\n",
    "    thu thập thông tin trong tương lai hay không. nếu không có callback thì mặc định là True, ngược lại là False\n",
    "\n",
    "LinkExtractor là một lớp đơn giản được thiết kế chỉ để nhận dạng và trả về các liên kết \n",
    "trong một trang nội dung HTML dựa trên các quy tắc được cung cấp cho nó.   \n",
    "Nó có một số đối số có thể được sử dụng để chấp nhận hoặc từ chối một liên \n",
    "kết dựa trên các bộ chọn, thẻ CSS và XPath\n",
    "\n",
    "Lớp LinkExtractor có thể thậm chí còn được mở rộng và có thể tạo các đối số tùy chỉnh\n",
    "chúng có các đối số thường dùng là:\n",
    "    - allow: Allow all links that match the provided regular expression.\n",
    "    - deny: Deny all links that match the provided regular expression\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Bằng cách sử dụng hai lớp Rule và LinkExtractor riêng biệt với một chức \n",
    "năng phân tích cú pháp duy nhất, bạn có thể tạo một trình thu thập thông \n",
    "tin thu thập dữ liệu Wikipedia, xác định tất cả các trang bài viết và các \n",
    "trang không phải bài viết đang gắn cờ (articlesMoreRules.py)'''\n",
    "\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "\n",
    "\n",
    "class ArticleSpider(CrawlSpider):\n",
    "    name='Chapter03_WritingWebCrawler.ipynb'\n",
    "    allowed_domain = ['wikipedia.org']\n",
    "    start_urls = ['https://en.wikipedia.org/wiki/Benevolent_dictator_for_life']\n",
    "    rules=[\n",
    "        Rule(LinkExtractor(allow='^(/wiki/)((?!:).)*$'), callback='parse_items', follow=True, cb_kwargs={'is_article':True}),\n",
    "        Rule(LinkExtractor(allow='.*'),callback='parse_items',cb_kwargs={'is_article':False})\n",
    "    ]\n",
    "\n",
    "    def parse_items(self, response, is_article):\n",
    "        print(response.url)\n",
    "        title = response.css('h1::text').extract_first()\n",
    "        if is_article:\n",
    "            url=response.url\n",
    "            text=response.xpath('//div[@id=\"mw-content-text\"]//text()').extract()\n",
    "            lastUpdated=response.css('li#footer-info-lastmod''::text').extract_first()\n",
    "            lastUpdated=lastUpdated.replace('This page was ''last edited on ','')\n",
    "            print('Title is: {} '.format(title))\n",
    "            print('title is: {} '.format(title))\n",
    "            print('text is: {}'.format(text))\n",
    "        else:\n",
    "            print('This is not an article: {}'.format(title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating items\n",
    "\n",
    "Scrapy  also  provides  useful  tools  to  keep  your  collected  items  organizedand stored in custom objects with well-defined fields.To help organize all the information you’re collecting, you need to create an Articleobject. Define a new item called Article inside the items.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outputing items\n",
    "\n",
    "$ scrapy runspider articleItems.py -o articles.csv -t csv\n",
    "\n",
    "$ scrapy runspider articleItems.py -o articles.json -t json\n",
    "\n",
    "$ scrapy runspider articleItems.py -o articles.xml -t xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the item pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logging with scrapy\n",
    "\n",
    "Thông tin gỡ lỗi do Scrapy tạo ra có thể hữu ích, nhưng, như bạn có thể đã nhận thấy, thông tin này thường quá dài dòng. Bạn có thể dễ dàng điều chỉnh mức độ ghi nhật ký bằng cách thêm aline vào tệp settings.py trong dự án Scrapy của mình\n",
    "\n",
    "LOG_LEVEL='ERROR'\n",
    "\n",
    "Scrapy sử dụng hệ thống phân cấp tiêu chuẩn của các cấp độ ghi nhật ký, như sau:\n",
    "\n",
    "    CRITICAL\n",
    "    ERROR\n",
    "    WARNING\n",
    "    DEBUG\n",
    "    INFO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
